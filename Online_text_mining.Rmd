---
author: "Jun"
date: "`r Sys.Date()`"
output: 
   prettydoc::html_pretty:
    toc : TRUE
    theme: leonids
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

<p align="center">
  <img src="https://www.08liter.com/assets/images/ico/img_logo_login_ko.png">
</p>

# 회사소개

경험 기반 소셜 커머스 회사로 "세상의 모든 판매자와 세상의 모든 소비자를 직접 만나게 하면 어떨까?" 라는 생각에서 출발하였으며, 고객이 SNS에 제품에 대한 리뷰를 남기는 대신 저렴한 비용으로 상품을 제공하는 서비스를 하고 있습니다. 

# 분석물 개요

검색어 '공팔리터'와 관련된 네이버 뉴스기사들을 크롤링하여, 공팔리터를 중심으로 어떤 키워드들이 같이 기술이 되었는지를 확인하기 위해 연관성 분석을 진행해보았습니다. 총 10페이지 분량의 기사들을 크롤링 하였습니다. 

# 필요한 라이브러리 활성화

```{r  message=FALSE, fig.align="center", fig.height=8, fig.width = 14}
library(rvest)
library(tidyverse)
library(KoNLP)
library(wordcloud2)
library(glue)
library(arules)
library(arulesViz)
```

# 데이터 전처리

```{r  message=FALSE, fig.align="center", fig.height=8, fig.width = 14}

keyword <- "공팔리터"

page_number <- seq(1, 100, 10)  

# map 함수 적용
map_news_urls <- map(page_number, function(x) {
  url <- glue("https://search.naver.com/search.naver?&where=news&query={keyword}&sm=tab_pge&sort=0&photo=0&field=0&reporter_article=&pd=0&ds=&de=&docid=&nso=so:r,p:all,a:all&mynews=0&cluster_rank=105&start={x}&refresh_start=0")
  
  news_urls <- url %>% 
    read_html() %>% 
    html_nodes(".news.mynews.section._prs_nws") %>% 
    html_nodes("._sp_each_url") %>% 
    html_attr("href")
  
  # 네이버 뉴스_index
  naver_news_index <- grep("https://news.naver.com/main", news_urls)
  
  # 네이버 뉴스 url만
  news_urls <- news_urls[naver_news_index]
}) %>% 
  unlist() %>% # 리스트에서 백터화
  unique()


news_text_set <- c()

for (i in 1:length(map_news_urls)) {
  news_text <- map_news_urls[i] %>%
    read_html() %>%
    html_nodes("#articleBodyContents._article_body_contents") %>%
    html_text()
  
  if(length(news_text) == 0) {
    text_set <- "내용 없음"
  }
  
  # 빈벡터에 뉴스 text를 하나씩 붙힘 
  news_text_set <- append(news_text, news_text_set)
  # Sys.sleep(1)
}


# 필요없는 문자열 제거
cleanging_data <- news_text_set %>% 
  str_remove_all("flash 오류를 우회하기 위한 함수 추가") %>% # 문자열 제거
  str_remove_all("function _flash_removeCallback") %>% 
  str_remove_all("[a-zA-Z]") %>% # 정규표현식 영어제거
  str_remove_all("\\d") %>% 
  str_remove_all("무단 전재 및 재배포 금지") %>% 
  str_remove_all("내용 없음") %>% 
  str_replace_all("\\W" , " ") # 정규표현식 : 문자가 아닌 것을 공백으로 변경


nouns <- map(cleanging_data,function(x) {
  # x 자리에 cleanging_data가 들어가 일괄적으로 처리함. 
 noun_data <- extractNoun(x) %>% 
   str_remove_all("은") %>% 
   str_remove_all("는") %>% 
   str_remove_all("가") %>% 
   str_remove_all("를") %>% 
   str_remove_all("일보")
 
 noun_data <- noun_data %>% 
   subset(nchar(noun_data) >= 2)
})


# 테이블 형태로 변환
word_count <- nouns %>% 
  unlist() %>% # 리스트를 벡터로 전환
  table() # 각 단어에 대한 빈도확인


# 편리하게 데이터 프레임으로 전환
df_word <- as.data.frame(word_count, stringsAsFactors = FALSE)

# 컬럼명 변경
colnames(df_word) <- c("word", "freq")


# 데이터 정제
word_freq <- df_word %>% 
  filter(freq >= 2) %>%
  arrange(desc(freq)) 


# 상위 n 개 데이터 파악
test_df <- word_freq %>% 
  top_n(20) %>% # 헤드함수와 같음
  select(word) %>% 
  t() %>%  # 행과 열의 위치 변경
  as.character()


item_list <- map(1:5, function(x){
  # x 자리에 1에서 5가 들어감.
  nouns[[x]] %>%
    subset(nouns[[x]] %in% test_df) %>%
    unique() %>%
    head(10)
})


# item의 트렌잭션화

item <- as(item_list, "transactions")


# apriori 함수 수행( 지지도 0.1, 신뢰도 0.8 이상인 연관성 규칙 구하기)
result_items <- apriori(item, parameter = list(support=0.1, confidence=0.8))

inspect(head(result_items, 5))


```


# 시각화

본 시각화 자료는 키워드 간의 연관성을 네트워크 구조로 표현한 자료입니다. '공팔리터'를 중심으로 '소비자', '경험', '솔직' 등 경험 기반 소셜커머스라는 이름에 걸맞는 상징적인 단어들이 연관 키워드로 나와있음을 확인 할 수 있습니다.
```{r  message=FALSE, fig.align="center", fig.height=8, fig.width = 14}

set.seed(200)
plot(result_items[1:80], method = "graph")

```

# 텍스트 마이닝

이 자료는 크롤링한 기사에서 사용된 단어들을 한데 모아놓은 것이며 각 단어의 크기는 단어사용 빈도수를 의미합니다. 역시 '소비자', '리뷰', '상품' 등 공팔리터의 정체성을 나타내는 단어들이 많이 사용되었음을 알 수 있습니다.
```{r  message=FALSE }
wordcloud2(word_freq,
           fontFamily = "Malgun Gothic",
           size = 1)
```

# 그래프화

위 자료에 이어서 뉴스에서 사용된 단어와 빈도 차이를 막대 그래프 형식으로 나타내고 있습니다. 
```{r  message=FALSE, fig.align="center", fig.height=8, fig.width = 14}
ggplot(head(word_freq,30), aes(x=reorder(word,freq),y=freq,fill=word),colour=gradient) +
  geom_col() +
  theme_bw() + 
  labs(x="",y="단어 빈도",title = paste0(keyword," 네이버 뉴스 단어 빈도")) + 
  coord_flip() + 
  theme(legend.position = "none") +
  theme(text = element_text(size=20),
        axis.text.x = element_text(angle=90, hjust=1))

```

